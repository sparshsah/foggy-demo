{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE BIAS-VARIANCE TRADEOFF IN RISK ESTIMATION\n",
    "\n",
    "author: [@sparshsah](https://github.com/sparshsah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting\n",
    "\n",
    "Suppose you have a series of observations $(r_1,\\, r_2,\\, \\dots,\\, r_{T})$ where each observation is i.i.d. Normal with ground-truth mean $\\mu$ and ground-truth variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators\n",
    "\n",
    "Consider three estimators for $\\sigma^2$. We're implicitly going to consider Mean Squared Error (MSE) as our loss function when evaluating them, but MSE isn't necessarily the best one. Which loss function is most appropriate can depend on the setting and application. For example, maybe in your particular use case, underestimating $\\sigma^2$ is more dangerous than overestimating.\n",
    "\n",
    "\n",
    "## Standard Bessel-Corrected Demeaned Sample Variance Estimator\n",
    "\n",
    "Define\n",
    "$$s^2_A := \\frac{1}{T-1} \\sum(r_t - \\bar{r})^2.$$\n",
    "\n",
    "This will be distributed as\n",
    "$$\\frac{1}{T-1}\\sigma^2\\chi^2_{T-1}.$$\n",
    "\n",
    "Its bias is $0$, so its squared bias is also $0$.\n",
    "\n",
    "Its squared standard error is $\\frac{1}{(T-1)^2}\\sigma^4 2(T-1) = 2\\frac{1}{T-1}\\sigma^4$.\n",
    "\n",
    "The sum of its squared bias plus squared standard error is\n",
    "$$2\\frac{1}{T-1}\\sigma^4.$$\n",
    "\n",
    "This has one undesirable property in the case I mentioned before: If underestimating $\\sigma^2$ is more dangerous than overestimating. This estimator will grossly underestimate, for instance, if all the $r$'s just randomly happen to come out to the same number.\n",
    "\n",
    "\n",
    "## Overriden Zero-Meaned Sample Variance Estimator\n",
    "\n",
    "Define\n",
    "$$s^2_B := \\frac{1}{T} \\sum r_t^2.$$\n",
    "\n",
    "This will be distributed as\n",
    "$$\\mu^2 + \\frac{1}{T}\\sigma^2\\chi^2_{T}.$$\n",
    "\n",
    "Its bias is $\\mu^2$, so its squared bias is $\\mu^4$.\n",
    "\n",
    "Its squared standard error is $\\frac{1}{T^2}\\sigma^4 2T = 2\\frac{1}{T}\\sigma^4$.\n",
    "\n",
    "The sum of its squared bias plus squared standard error is\n",
    "$$\\mu^4 + 2\\frac{1}{T}\\sigma^4.$$\n",
    "\n",
    "\n",
    "## Minimum-MSE Sample Variance Estimator\n",
    "\n",
    "Define\n",
    "$$s^2_C := \\frac{1}{T+1} \\sum(r_t - \\bar{r})^2.$$\n",
    "\n",
    "This is the best you can do in terms of MSE [[cf](https://web.archive.org/web/20210522072302/https://en.wikipedia.org/wiki/Mean_squared_error#Variance)], but I'm not sure what its distribution is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE comparison\n",
    "\n",
    "Let's compare $s^2_B$ vs $s^2_A$. When will the overriden estimator's sum of squared bias plus squared standard error be better (i.e. smaller) than the standard's?\n",
    "\n",
    "Well, when\n",
    "$$\\mu^4 + 2\\frac{1}{T}\\sigma^4 < 2\\frac{1}{T-1}\\sigma^4$$\n",
    "$$\\mu^4 < 2\\left(\\frac{1}{T-1} - \\frac{1}{T}\\right)\\sigma^4$$\n",
    "$$\\mu^4 < 2\\frac{1}{(T-1)T}\\sigma^4$$\n",
    "$$\\mu < \\sqrt[4]{2\\frac{1}{(T-1)T}}\\sigma$$\n",
    "$$\\frac{\\mu}{\\sigma} < \\sqrt[4]{2\\frac{1}{(T-1)T}}.$$\n",
    "\n",
    "\n",
    "## Upshot\n",
    "\n",
    "For example, if $T = 65$ days, then $s^2_B$ will have a lower sum-of-squared-bias-plus-squared-standard-error (i.e. lower MSE) than $s^2_A$ as long as the ratio of $\\mu$ to $\\sigma$ (each for a single observation, i.e. a single day) is less than $\\approx 0.148$ (i.e. a daily Sharpe less than $\\approx 0.148$). In other words: With a single business quarter of daily-returns data, the zero-meaned estimator would be better (from an MSE perspective) than the demeaned estimator as long as the asset's ground-truth business-annualized Sharpe was less than $\\approx 261^{0.5} \\cdot 0.148 = 2.39$. A business quarter is a reasonable and popular estimation horizon (to deal with the fact that market data-generating processes are highly non-stationary), and most assets' ground-truth annualized Sharpes are much less than $2.39$, so this is a pretty common scenario.\n",
    "\n",
    "_On the other hand_: Even if you had a full business century ($T = 100 \\cdot 261 = 26,100$ days) of daily-returns data, the zero-meaned estimator would _still_ be better as long as the asset's ground-truth daily Sharpe was less than $\\approx 0.0074$, which annualizes to $\\approx 261^{0.5} \\cdot 0.0074 \\approx 0.12$ -- A surprisingly high figure in my eyes. Put another way: There are commodities out there whose ground-truth annualized Sharpes are widely assumed to be around $0.10$. This result says that even if you had high-quality daily returns data going back to 1922, you should _still_ use the zero-meaned variance estimator if you want to get lower expected squared estimation error.\n",
    "\n",
    "\n",
    "## An interesting observation: Asymptotic frequency-independence\n",
    "\n",
    "I'm going to do a blind-plug-and-chug exercise here.\n",
    "\n",
    "Take $T = 26,100$. We get crossover at a $\\mu$-to-$\\sigma$ ratio of $0.0074$. Now take $T = 100$. We get crossover at a $\\mu$-to-$\\sigma$ ratio of $0.12$.\n",
    "\n",
    "Ok. Let's interpret each observation in the first case as being a single day's return. That means our daily-mean-to-daily-volatility ratio, i.e. our daily Sharpe ratio, must be less than $0.0074$. Annualizing, we get an annualized Sharpe ratio of $261^{0.5} * 0.0074 = 0.12$.\n",
    "\n",
    "Now let's interpret each observation in the second case as being a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# https://github.com/sparshsah/foggy-lib/tree/main/util\n",
    "sys.path.append(\"../../../foggy-lib/util/\")\n",
    "del sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# https://github.com/sparshsah/foggy-lib/blob/main/util/foggy_pylib/core.py\n",
    "import foggy_pylib.core as fc\n",
    "# https://github.com/sparshsah/foggy-lib/blob/main/util/foggy_pylib/fin.py\n",
    "import foggy_pylib.fin as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the crossover point decay in sample size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
