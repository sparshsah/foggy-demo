{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Joint vs Individual Linear Regression Loadings\n",
    "\n",
    "First, some notation. Be warned that I am going to take advantage of the typographical similarity between certain Greek and Roman letters (classicists and pedants, avert thy eyes!). Other than that, however, my notation should be very natural to anyone who's taken a college-level linear models course.\n",
    "\n",
    "Afterward, I'll run some experiments where I prove (or at least state) some theoretical (ground-truth) results about the \"Greek\" quantities, then demonstrate them using their \"Roman\" counterparts calculated on toy datasets. To give a trivial example, to demonstrate that $\\sigma(\\gamma) \\geq 0$, I could show that $s(y) = 0.42$ (which is nonnegative) for some specific toy dataset.\n",
    "\n",
    "\n",
    "## The Greeks: Ground-Truth Data-Generating Process\n",
    "\n",
    "Let $\\gamma,\\, \\chi_1,\\, \\chi_2,\\, \\varepsilon$ be a \"data-generating\" or \"stochastic\" process (basically, a vector of real-valued random variables) such that $\\gamma = \\beta_0 + \\beta_1\\chi_1 + \\beta_2\\chi_2 + \\varepsilon$ where $\\varepsilon$ is white noise with each draw independent of the other(s).\n",
    "\n",
    "Let $\\sigma(\\cdot)$ represent standard deviation, $\\sigma^2(\\cdot)$ represent variance, $\\sigma^2(\\cdot,\\, \\cdot)$ represent covariance, and $\\rho(\\cdot,\\, \\cdot)$ represent correlation. Let $\\Sigma$ be the variance-covariance matrix between $\\chi_1$ and $\\chi_2$ i.e.\n",
    "$$\\begin{pmatrix} \\sigma^2(\\chi_1) & \\sigma^2(\\chi_1,\\, \\chi_2) \\\\ \\sigma^2(\\chi_1,\\, \\chi_2) & \\sigma^2(\\chi_2)\\end{pmatrix},$$ and $\\Omega$ be the corresponding correlation matrix.\n",
    "\n",
    "Importantly, assume that $|\\rho(\\chi_1,\\, \\chi_2)| \\neq 1$.\n",
    "\n",
    "\n",
    "## The Romans: Practical Calculations On Observed Data\n",
    "\n",
    "Suppose we observe $N$ different \"draws\" or \"samples\" from this process. Arrange the draws of $\\gamma$ into a column vector of real numbers $y := [y_n]$, the draws of $\\chi_1$ into a column vector of real numbers $x_1 := [x_{n,1}]$, the draws of $\\chi_2$ into a column vector of real numbers $x_2 := [x_{n,2}]$, and the draws of $\\varepsilon$ into a column vector of real numbers $e := [e_n]$, over $n \\in [1,\\, N]$. Further, arrange the $x$'s into an $N \\times 3$ matrix of real numbers $X := [1,\\, x_1,\\, x_2]$ whose first column is all ones (AKA \"constant\" AKA \"intercept\").\n",
    "\n",
    "Let the vector of real numbers $b := [b_0,\\, b_1,\\, b_2] := (X^\\top X)^{-1} X^\\top y$ be the coefficients from an OLS regression of $y$ onto $X$ [1]. More generally, define $\\texttt{ols}$ such that e.g. $\\texttt{ols}(y,\\, [1,\\, x_1,\\, x_2]) := [b_0, b_1, b_2] =: b$.\n",
    "\n",
    "Finally, let $s$, $s^2$, $r$, $S$, and $U$ be the usual Bessel-corrected \"sample\" estimators of their \"population\" counterparts in Greek above.\n",
    "\n",
    "\n",
    "Footnotes\n",
    "---------\n",
    "[1] Take this formula for granted. Recall that if $\\varepsilon$ is Normally distributed, then OLS yields the MLE for $\\beta$ given $y,\\, X$. On the other hand if $\\varepsilon$ is Laplace-distributed, then instead LAD would yield the MLE."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "const   -0.0\n",
       "x1       1.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def gen_data(mean: Tuple[float]=(0, 0), std: Tuple[float]=(1, 1), corr: float=0, n: int=10_000_000, seed: int=42):\n",
    "    \"\"\"\n",
    "    Generate a toy dataset where y = x1 + x2 + white_noise.\n",
    "\n",
    "    input\n",
    "    -----\n",
    "    mean: tuple[float] (default 0), ground-truth means of the x's.\n",
    "    std: tuple[float] (default 1), ground-truth standard deviations of the x's.\n",
    "    corr: float (default 0), ground-truth correlation between the x's.\n",
    "    n: int (default 10 million), number of data points to generate.\n",
    "    seed: int (default 42), random seed.\n",
    "\n",
    "    output\n",
    "    ------\n",
    "    pd.DataFrame(columns=[\"y\", \"x1\", \"x2\", \"x1+x2\", \"white_noise\"]).\n",
    "    \"\"\"\n",
    "    mean = pd.Series({\"x1\": mean[0], \"x2\": mean[1], \"white_noise\": 0})\n",
    "\n",
    "    std = pd.Series({\"x1\": std[0], \"x2\": std[1], \"white_noise\": 1})\n",
    "    # diagonal matrix with std's on the diagonal\n",
    "    std_ = pd.DataFrame(np.diag(std), index=std.index, columns=std.index)\n",
    "\n",
    "    corr = pd.DataFrame({\n",
    "        \"x1\": {\"x1\": 1, \"x2\": corr, \"white_noise\": 0},\n",
    "        \"x2\": {\"x1\": corr, \"x2\": 1, \"white_noise\": 0},\n",
    "        \"white_noise\": {\"x1\": 0, \"x2\": 0, \"white_noise\": 1}\n",
    "    })\n",
    "\n",
    "    cov = std_ @ corr @ std_\n",
    "\n",
    "    np.random.seed(seed=seed)\n",
    "    df = pd.DataFrame(np.random.multivariate_normal(mean=mean, cov=cov, size=n), columns=mean.index)\n",
    "    df.loc[:, \"x1+x2\"] = df[\"x1\"] + df[\"x2\"]\n",
    "    df.loc[:, \"y\"] = df[\"x1+x2\"] + df[\"white_noise\"]\n",
    "    return df[[\"y\", \"x1\", \"x2\", \"x1+x2\", \"white_noise\"]]\n",
    "\n",
    "\n",
    "def ols(y, x, decimals=2):\n",
    "    \"\"\"Get OLS coefficient vector.\"\"\"\n",
    "    return np.round(sm.OLS(endog=y, exog=sm.add_constant(x), hasconst=True).fit().params, decimals=decimals)\n",
    "\n",
    "\n",
    "# example\n",
    "df = gen_data()\n",
    "ols(y=df[\"y\"], x=df[\"x1\"])"
   ]
  },
  {
   "source": [
    "## Result 0.0\n",
    "\n",
    "Suppose $\\rho(\\chi_1,\\, \\chi_2) = 0$. Then, $\\beta_i = \\sigma^{-2}(\\chi_i)\\sigma^2(\\chi_i,\\, \\gamma)$. This is the familiar formula for a univariate regression slope, otherwise stated as $\\beta_i = \\frac{\\texttt{Cov}(\\chi_i,\\, \\gamma)}{\\texttt{Var}(\\chi_i)}$. Notice how similar this looks to the $(X^\\top X)^{-1} X^\\top y$ formula.\n",
    "\n",
    "### Corollary 0.0.C\n",
    "Let $[a_0,\\, a_1] := \\texttt{ols}(y,\\, [1, x_1])$, $[b_0,\\, b_2] := \\texttt{ols}(y,\\, [1, x_2])$, $[c_0,\\, c_1,\\, c_2] := \\texttt{ols}(y,\\, [1, x_1,\\, x_2])$. We will have $c_0 = a_0 + b_0$, $c_1 = a_1$, and $c_2 = b_2$ iff $r(x_1,\\, x_2) = 0$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Result 0.0.1\n",
    "\n",
    "Suppose further that $\\texttt{E}(\\chi_1) = 0 = \\texttt{E}(\\chi_2)$. Then, $[\\beta_1,\\, \\beta_2] = \\Sigma^{-1}\\Sigma_\\gamma$ where $\\Sigma_\\gamma$ the is the column vector $[\\sigma^2(\\chi_1,\\, \\gamma) ,\\, \\sigma^2(\\chi_2,\\, \\gamma)]$ of covariances between the $\\chi$'s and $\\gamma$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Result 0.1\n",
    "\n",
    "Suppose $\\rho(\\chi_1,\\, \\chi_2) \\neq 0$. Then, the conclusion of Result 0.0 will not hold."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Result 1\n",
    "\n",
    "Let $[a_0,\\, a_1] := \\texttt{ols}(y,\\, [1, x_1])$, $[b_0,\\, b_2] := \\texttt{ols}(y,\\, [1, x_2])$, $[c_0,\\, c_{1+2}] := \\texttt{ols}(y,\\, [1, x_1+x_2])$. Under what conditions will we have $c_0 = a_0 + b_0$ and $c_{1+2} = a_1 + b_2$?\n",
    "\n",
    "Let's start backward:\n",
    "$$a_1 = \\frac{s^2(x_1,\\, y)}{s^2(x_1)}$$\n",
    "$$b_2 = \\frac{s^2(x_2,\\, y)}{s^2(x_2)}$$\n",
    "$$a_1 + b_2 = \\frac{s^2(x_1,\\, y)}{s^2(x_1)} + \\frac{s^2(x_2,\\, y)}{s^2(x_2)} = \\frac{s^2(x_2)s^2(x_1,\\, y) + s^2(x_1)s^2(x_2,\\, y)}{s^2(x_1)s^2(x_2)}$$\n",
    "$$c_{1+2} = \\frac{s^2(x_1 + x_2,\\, y)}{s^2(x_1 + x_2)} = \\frac{s^2(x_1,\\, y) + s^2(x_2,\\, y)}{s^2(x_1) + s^2(x_2) + 2s^2(x_1,\\, x_2)}$$\n",
    "Hence we want\n",
    "$$\\frac{s^2(x_2)s^2(x_1,\\, y) + s^2(x_1)s^2(x_2,\\, y)}{s^2(x_1)s^2(x_2)} = \\frac{s^2(x_1,\\, y) + s^2(x_2,\\, y)}{s^2(x_1) + s^2(x_2) + 2s^2(x_1,\\, x_2)}$$\n",
    "Clearly this is underidentified so let's assert that $s(x_1) = s = s(x_2)$, whereby we get\n",
    "$$\\frac{s^2(x_1,\\, y) + s^2(x_2,\\, y)}{s^2} = \\frac{s^2(x_1,\\, y) + s^2(x_2,\\, y)}{2s^2 + 2s^2(x_1,\\, x_2)}$$\n",
    "Now the numerators match, so we just need to make the denominators match\n",
    "$$s^2 = 2s^2 + 2s^2(x_1,\\, x_2) = 2s^2 + 2r(x_1,\\, x_2)s^2 = 2(1 + r(x_1,\\, x_2))s^2$$\n",
    "$$1 = 2(1 + r(x_1,\\, x_2))$$\n",
    "$$-0.5 = r(x_1,\\, x_2)$$\n",
    "So if their standard deviations are the same, we need $x_1$ and $x_2$ to be $-0.5$ correlated if we want to slopes to add up."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}