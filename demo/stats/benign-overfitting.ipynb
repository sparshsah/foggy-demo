{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BENIGN OVERFITTING\n",
    "\n",
    "This is inspired by Peter Bartlett's [lecture](https://youtu.be/GXpP-rXEDpk), but it's not a direct demonstration of his claim. His claim is already proven, so I want to instead illustrate the essential idea in an extremely (read: \"uselessly\") simple setting, just to gain intuition.\n",
    "\n",
    "The idea is that the Moore-Penrose pseudoinverse gives rise to a linear regression estimator that, among all best-fitting $\\beta$'s, chooses the one with smallest norm. When the number of regressors is smaller than the number of observations, this recovers the usual (unique) OLS estimator. But when it is much larger, the output is something very similar-looking to a ridge-regularized linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "import sklearn.linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-generating process\n",
    "\n",
    "Exceedingly simple. There are `N` raw-data feeds. Each feed is an i.i.d. standard Normal random variable. Of these, `N` feeds, `M` are actually relevant to our `y`-generating process, in the simplest way: `y` is the sum of the `M` relevant data feeds, plus white noise. Exactly which `M` are relevant is random. We have `T` observations in the training sample, and, because it doesn't matter, also `T` in the test sample.\n",
    "\n",
    "We have $M \\ll N$ and also $T \\ll N$, but the relevant thing here is that $T \\ll N$. Usually, we'd say that blindly using all `N` raw-data feeds is a terrible idea, because we'll be able to perfect interpolate (i.e. perfect overfit!) the training data. BUT... is that conventional wisdom actually true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSubsample = Tuple[pd.DataFrame, pd.Series]\n",
    "# train, test\n",
    "DataSample = Tuple[DataSubsample, DataSubsample]\n",
    "\n",
    "N = 10_000\n",
    "M = int(N/10)\n",
    "T = int(N/100)\n",
    "NOISE = 1\n",
    "\n",
    "\n",
    "def _get_x_name(base: str=\"x\", n: int=0) -> str:\n",
    "    return f\"{base}_{n}\"\n",
    "\n",
    "def ___gen_x(T: int=T) -> pd.Series:\n",
    "    \"\"\"Generate observations for a single raw-data feed.\"\"\"\n",
    "    x = stats.norm.rvs(size=T)\n",
    "    x = pd.Series(x)\n",
    "    return x\n",
    "\n",
    "def __gen_X(N: int=N, T: int=T) -> pd.DataFrame:\n",
    "    \"\"\"Generate observations for all raw-data feeds.\"\"\"\n",
    "    X = {_get_x_name(n): ___gen_x() for n in range(N)}\n",
    "    X = pd.DataFrame(X)\n",
    "    assert X.shape == (T, N), X.shape\n",
    "    return X\n",
    "\n",
    "def _pick_relevant_x(N: int=N, M: int=M) -> pd.Series:\n",
    "    \"\"\"beta[n] indicates whether X[:,n] is relevant.\"\"\"\n",
    "    beta_star = pd.Series(0, index=range(N))\n",
    "    # detereministically pick the first `M` of these, just so we \n",
    "    # have the correct total number of \"live\" picks to distribute\n",
    "    beta_star.iloc[:M] = 1\n",
    "    # distribute the picks uniformly randomly\n",
    "    beta_star = beta_star.sample(n=N).reset_index(drop=True)\n",
    "    beta_star = beta_star.rename(index=_get_x_name)\n",
    "    assert beta_star.sum() == M, beta_star.sum()\n",
    "    return beta_star\n",
    "\n",
    "def __get_y(beta_star: pd.Series, X: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Generate y's, where beta_star indicates whether a column is relevant.\"\"\"\n",
    "    # pick out the relevant x's, zero out the rest\n",
    "    y = X * beta_star\n",
    "    # y is a simple sum of the relevant raw-data feeds\n",
    "    y = y.sum(axis=\"columns\")\n",
    "    # add noise\n",
    "    y = y + stats.norm.rvs(scale=NOISE, size=T)\n",
    "    y.name = \"y\"\n",
    "    return y\n",
    "\n",
    "\n",
    "def _gen_data_subsample(beta_star: pd.Series) -> DataSubsample:\n",
    "    \"\"\"Get X, y.\"\"\"\n",
    "    X = __gen_X()\n",
    "    y = __get_y(beta_star=beta_star, X=X)\n",
    "    return X, y\n",
    "\n",
    "def gen_data_sample() -> Tuple[pd.Series, DataSample]:\n",
    "    beta_star = _pick_relevant_x()\n",
    "    # train subsample\n",
    "    X, y = _gen_data_subsample(beta_star=beta_star)\n",
    "    train_subsample = X, y\n",
    "    # test subsample\n",
    "    X_, y_ = _gen_data_subsample(beta_star=beta_star)\n",
    "    test_subsample = X_, y_\n",
    "    sample = train_subsample, test_subsample\n",
    "    return beta_star, sample\n",
    "\n",
    "np.random.seed(1337)\n",
    "# M relevant x's, training subsample, testing subsample\n",
    "beta_star, ((X, y), (X_, y_)) = gen_data_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature creation\n",
    "\n",
    "To show how performance varies with increasing degree of overfitting, we need to increase the degree of overfitting. To do this in the linear regression setting, we can just increase the number of regressors until it equals---and indeed even further until it is much larger than---the number of observations. We'll do this in a very simple way: If we need `R` regressors, we'll use the first `R` columns of `X`. Notice that this doesn't matter: The `N` raw-data feeds were i.i.d. random, and which `M` of the `N` raw-data feeds were chosen to be relevant was also uniformly random. So shuffling the `N` raw-data feeds before choosing `R` of them is unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# this implementation uses\n",
    "# (a flavor of) random Fourier features,\n",
    "# which would be much more useful for actual prediction.\n",
    "# however, it's already getting to complicated\n",
    "# for my liking,\n",
    "# and IMO obscures the magic.\n",
    "\n",
    "def _get_rhs(X: pd.DataFrame) -> pd.Series:\n",
    "    w = stats.norm.rvs(size=len(X.columns))\n",
    "    w = pd.Series(w, index=X.columns)\n",
    "    rhs = X * w\n",
    "    rhs = rhs.sum(axis=\"columns\")\n",
    "    rhs = np.sin(rhs)\n",
    "    return rhs\n",
    "\n",
    "def get_RHS(X: pd.DataFrame, R: int) -> pd.DataFrame:\n",
    "    RHS = {_get_x_name(base=\"r\", n=r): _get_rhs(X=X) for r in range(R)}\n",
    "    RHS = pd.DataFrame(RHS)\n",
    "    return RHS\n",
    "\"\"\"\n",
    "\n",
    "def get_RHS(X: pd.DataFrame, R: int) -> pd.DataFrame:\n",
    "    return X.iloc[:, :R]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIDGE_REGULARIZATION = 1\n",
    "\n",
    "\n",
    "def __fit_star_(**kwargs) -> pd.Series:\n",
    "    \"\"\"Ignored args and returns beta_star.\"\"\"\n",
    "    return beta_star\n",
    "\n",
    "def __fit_bo(y: pd.Series, RHS: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Benignly-overfit model:\n",
    "    (X'X)^{-1} X'y, where ^{-1} denotes Moore-Penrose pseudoinverse.\n",
    "    This is OLS if len(X.columns) <= len(y).\n",
    "    \"\"\"\n",
    "    X_T_X = RHS.T @ RHS\n",
    "    X_T_X_inv = pinv(X_T_X)\n",
    "    X_T_y = RHS.T @ y\n",
    "    beta_hat = X_T_X_inv @ X_T_y\n",
    "    beta_hat = pd.Series(beta_hat, index=RHS.columns)\n",
    "    return beta_hat\n",
    "\n",
    "def __fit_ridge(\n",
    "        y: pd.Series, RHS: pd.DataFrame,\n",
    "        regularization: float=RIDGE_REGULARIZATION\n",
    "    ) -> pd.Series:\n",
    "    \"\"\"This is OLS if penalty == 0.\"\"\"\n",
    "    model = lm.Ridge(alpha=regularization, fit_intercept=False)\n",
    "    model = model.fit(y=y, X=RHS)\n",
    "    beta_hat = pd.Series(model.coef_, index=RHS.columns)\n",
    "    return beta_hat\n",
    "    \n",
    "\n",
    "def _fit_model(y: pd.Series, RHS: pd.DataFrame, kind: str=\"star\") -> pd.Series:\n",
    "    \"\"\"Get beta_hat.\"\"\"\n",
    "    _fit = {\n",
    "        \"star\": __fit_star_,\n",
    "        \"bo\": __fit_bo,\n",
    "        \"ridge\": __fit_ridge\n",
    "    }[kind]\n",
    "    beta_hat = _fit(y=y, RHS=RHS)\n",
    "    return beta_hat\n",
    "\n",
    "def fit_model(\n",
    "        y: pd.Series, X: pd.DataFrame, R: int, kind: str=\"star\"\n",
    "    ) -> pd.Series:\n",
    "    RHS = get_RHS(X=X, R=R)\n",
    "    beta_hat = _fit_model(y=y, RHS=RHS, kind=kind)\n",
    "    # fill the potentially-empty var's\n",
    "    beta_hat = beta_hat.reindex(index=X.columns).fillna(0)\n",
    "    return beta_hat\n",
    "\n",
    "def fit_model_(R: int, kind: str=\"star\"):\n",
    "    return fit_model(y=y, X=X, R=R, kind=kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_y_hat(beta: pd.Series, X: pd.DataFrame) -> pd.Series:\n",
    "    y_hat = X * beta\n",
    "    y_hat = y_hat.sum(axis=\"columns\")\n",
    "    return y_hat\n",
    "\n",
    "def __get_loss(y: pd.Series, y_hat: pd.Series) -> float:\n",
    "    \"\"\"RMSE.\"\"\"\n",
    "    loss = y_hat - y\n",
    "    loss = loss**2\n",
    "    loss = loss.mean()\n",
    "    loss = loss**0.5\n",
    "    return loss\n",
    "\n",
    "def _get_loss(y: pd.Series, X: pd.DataFrame, beta: pd.Series) -> float:\n",
    "    y_hat = __get_y_hat(beta=beta, X=X)\n",
    "    return __get_loss(y=y, y_hat=y_hat)\n",
    "\n",
    "def get_loss(\n",
    "        y: pd.Series, X: pd.DataFrame,  # train subsample\n",
    "        y_: pd.Series, X_: pd.Series,  # test subsample\n",
    "        R: int,\n",
    "        kind: str=\"star\"\n",
    "    ) -> float:\n",
    "    \"\"\"Fit specified model and get test-subsample loss.\"\"\"\n",
    "    beta_hat = fit_model(y=y, X=X, R=R, kind=kind)\n",
    "    loss = _get_loss(y=y_, X=X_, beta=beta_hat)\n",
    "    return loss\n",
    "\n",
    "def get_loss_(R: int, kind: str=\"star\") -> float:\n",
    "    return get_loss(y=y, X=X, y_=y_, X_=X_, R=R, kind=kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "In this oversimplified toy setup, the actual test-subsample performance of the models is almost indistinguishable from (indeed worse than) a model that simply ignores the $x$'s and guesses $0$ every time (`flat loss`), and nowhere near the performance of the ground-truth model (`star loss`).\n",
    "\n",
    "Nevertheless, it shows the point: As $R \\to T$, that is, as the number of regressors gets very close to the number of observations, we suffer from a singularity issue, in particular when $R$ approaches $T$ from above. The RMSE of the OLS-type estimator blows up (`bo loss`), and we see the canonical benefit of regularization (in this case, a no-thought ridge model) (`ridge loss`). Yet, when $R \\gg T$, that is, when the number of regressors is much greater than the number of observations, the OLS-type model actually becomes indistinguishable from the regularized model! So, thanks to the Moore-Penrose pseudoinverse, as we really crank up the complexity (dimensionality) of our model, we get a natural implicit regularization, without ever having to consciously choose any regularization parameters.\n",
    "\n",
    "\n",
    "Note: To actually get some good performance out of this, I think we could make the methodology closer to Bartlett's, e.g. use random Fourier features to have a better chance of \"blending in\" some of the predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=0:\n",
      "star loss = 1.017412008907012\n",
      "flat loss = 30.54245569842787\n",
      "\n",
      "R=1:\n",
      "bo loss = 30.866563472420818\n",
      "ridge loss = 30.861803801717798\n",
      "\n",
      "R=5:\n",
      "bo loss = 32.93418422764194\n",
      "ridge loss = 32.90684156832083\n",
      "\n",
      "R=50:\n",
      "bo loss = 41.344341229732294\n",
      "ridge loss = 40.79174373136495\n",
      "\n",
      "R=99:\n",
      "bo loss = 205.9078525698844\n",
      "ridge loss = 80.08616246165958\n",
      "\n",
      "R=100:\n",
      "bo loss = 504.68851633075343\n",
      "ridge loss = 80.47778433265394\n",
      "\n",
      "R=101:\n",
      "bo loss = 403.8034618866445\n",
      "ridge loss = 78.47148590195188\n",
      "\n",
      "R=550:\n",
      "bo loss = 34.891655936046845\n",
      "ridge loss = 34.87125980639601\n",
      "\n",
      "R=1000:\n",
      "bo loss = 33.08875089562013\n",
      "ridge loss = 33.083695233986575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"R=0:\")\n",
    "print(f\"star loss = {get_loss_(R=None)}\")\n",
    "print(f\"flat loss = {(y_**2).mean()**0.5}\")\n",
    "print()\n",
    "\n",
    "for R in (1, 5, int(T/2), T-1, T, T+1, int((T+M)/2), M):\n",
    "    print(f\"R={R}:\")\n",
    "    for kind in (\"bo\", \"ridge\"):\n",
    "        print(f\"{kind} loss = {get_loss_(R=R, kind=kind)}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
