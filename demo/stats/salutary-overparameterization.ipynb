{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTHOR: SPARSHSAH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SALUTARY OVERPARAMETERIZATION IN A SIMPLE LINEAR-REGRESSION SETTING\n",
    "\n",
    "This is inspired by Peter Bartlett's [benign-overfitting lecture](https://youtu.be/GXpP-rXEDpk), but it's not a direct demonstration of his claim. His claim is already proven, so I want to instead illustrate the essential idea in an extremely (read: \"uselessly\") simple setting, just to gain intuition.\n",
    "\n",
    "\n",
    "## Recap: What do we know about linear regression?\n",
    "\n",
    "We know that the usual \"ridgeless\" OLS estimator gives the unique solution $\\quad [\\, (RHS)'(RHS) \\,]^{-1} \\; (RHS)'y \\quad$ when $R$ (the number of regressors, i.e. the number of the columns in the $RHS$ matrix) is no larger than $T$ (the number of observations), and gives a multiplicity of equally-well-fitting (indeed perfectly-fitting) solutions when $R > T$.\n",
    "\n",
    "We also know that, despite training error dropping to zero as soon as $R \\geq T$, the resulting \"overfit\" model does not generally hold up well out-of-sample. So, we can either reduce the dimensionality of the feature space (e.g. PCA on $RHS$, keeping only $R' < T$ of the resulting PC's of the regressors), or regularize the estimator with e.g. ridge.\n",
    "\n",
    "### Making the solution unique in the overparameterized case\n",
    "\n",
    "Now, let's just tweak the \"ridgeless\" solution, and define it to be $\\quad \\text{pinv}[\\, (RHS)'(RHS) \\,] \\; (RHS)'y \\quad$ where $\\text{pinv}$ is the Moore-Penrose pseudoinverse. This is the same when $R \\leq T$, but importantly it gives a unique solution even when $R > T$. In particular it chooses among all $\\beta$'s that fit the training data perfectly, the unique such $\\beta$ with smallest norm.\n",
    "\n",
    "And, we'll also just observe that ridge regression (with positive regularization, that is, positive penalty factor) also has a well-specified and unique solution even when $R > T$. Why? Well, the ridge of $\\lambda$'s added along the diagonal of $(RHS)'(RHS)$ turns \"equivalent\" (linearly-dependent) rows into linearly-independent rows, thereby making the matrix invertible.\n",
    "\n",
    "\n",
    "## Tautology: Implicit regularization is regularization\n",
    "\n",
    "There's nothing revolutionary in this part. We're not going to dispute that when $R$ is close to $T$, the ridgeless (indeed, even the ridge) solution holds up quite poorly out-of-sample. In fact, we're not even going to claim that when $R > T$ either the ridgeless or the ridge solution necessarily holds up well out-of-sample.\n",
    "\n",
    "However, we _will_ observe that as $R$ becomes much larger than $T$, the ridgeless solution becomes indistinguishable from the ridge solution. This is not a shock, as we already characterized the Moore-Penrose pseudoinverse as giving rise to a \"implicitly\" regularized $\\beta$. This isn't revolutionary, but it's a good stepping-stone to the next part -- which _is_ revolutionary.\n",
    "\n",
    "\n",
    "## Upshot: Benefit of aggressive overparameterization\n",
    "\n",
    "So, we've conceded that when the number of parameters is close to the number of observations, both the ridgeless and the ridge model tend to fare pretty poorly (unless you apply aggressive regularization to the ridge model). And, we've established that asymptotically, the ridgeless and ridge model become pretty much indistinguishable. So for this next statement, choose whichever one you'd like. Here's the shock: As $R$ becomes much larger than $T$, the model begins to recover, and the $R \\gg T$ cases give much, much smaller test error than the $R=T$ case!\n",
    "\n",
    "Thus, the fact that highly \"overparameterized\" deep neural nets can achieve zero training error yet still perform well out-of-sample, might not be some mystery confined to deep learning only. We don't know for sure that the same pathways are active here, but we do see that even in the linear-regression setting, neither an implicitly- nor an explicitly-regularized solution can avoid catastrophic badness out-of-sample when $R$ (the number of parameters) is near $T$; Yet, both recover when $R \\gg T$! So, maybe the key in deep learning is that it aggressively overparameterizes the model, and therefore gets good performance out-of-sample despite overfitting in-sample.\n",
    "\n",
    "So, I go a step further than Bartlett: I don't call this \"benign overfitting\", but rather \"salutary overparameterization\"! Yes, the _in-sample_ overfitting when the model is aggressively overparameterized is benign: But I'd emphasize that the benefit of aggressive overparameterization to accuracy _out-of-sample_ is the real shock!\n",
    "\n",
    "\n",
    "## Takeaway: Don't be afraid to use the information you have\n",
    "\n",
    "Note that, from the original paper (and, enforced below), these results require that predictiveness is \"uniformly distributed\" among the $x$'s. So, this isn't carte-blanche blessing to throw random garbage into your regression model. But, it does say that, as long as your regressors \"make sense\" (take this in a Bayesian-prior way), you shouldn't necessarily be afraid to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "import sklearn.linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-generating process\n",
    "\n",
    "Exceedingly simple. There are `N` raw-data feeds. Each feed is an i.i.d. standard Normal random variable. Of these, `N` feeds, `M` are actually relevant to our `y`-generating process, in the simplest way: `y` is the sum of the `M` relevant data feeds, plus white noise. Exactly which `M` are relevant is random. We have `T` observations in the training sample, and, because it doesn't matter, also `T` in the test sample.\n",
    "\n",
    "We have $M \\ll N$ and $T \\lll N$. Now usually, we'd say that blindly using all `N` raw-data feeds is a terrible idea, because we'll be able to perfect interpolate (i.e. perfect overfit!) the training data. BUT... is that conventional wisdom actually true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSubsample = Tuple[pd.DataFrame, pd.Series]\n",
    "# train, test\n",
    "DataSample = Tuple[DataSubsample, DataSubsample]\n",
    "\n",
    "N = 100_000\n",
    "M = int(N * 0.01)\n",
    "T = int(N * 0.001)\n",
    "NOISE = 1\n",
    "\n",
    "\n",
    "def _get_x_name(base: str=\"x\", n: int=0) -> str:\n",
    "    return f\"{base}_{n}\"\n",
    "\n",
    "def ___gen_x(T: int=T) -> pd.Series:\n",
    "    \"\"\"Generate observations for a single raw-data feed.\"\"\"\n",
    "    x = stats.norm.rvs(size=T)\n",
    "    x = pd.Series(x)\n",
    "    return x\n",
    "\n",
    "def __gen_X(N: int=N, T: int=T) -> pd.DataFrame:\n",
    "    \"\"\"Generate observations for all raw-data feeds.\"\"\"\n",
    "    X = {_get_x_name(n): ___gen_x() for n in range(N)}\n",
    "    X = pd.DataFrame(X)\n",
    "    assert X.shape == (T, N), X.shape\n",
    "    return X\n",
    "\n",
    "def _pick_relevant_x(N: int=N, M: int=M) -> pd.Series:\n",
    "    \"\"\"beta[n] indicates whether X[:,n] is relevant.\"\"\"\n",
    "    beta_star = pd.Series(0, index=range(N))\n",
    "    # deterministically pick the first `M` of these, just so we \n",
    "    # have the correct total number of \"live\" picks to distribute\n",
    "    beta_star.iloc[:M] = 1\n",
    "    # distribute the picks uniformly randomly\n",
    "    beta_star = beta_star.sample(n=N).reset_index(drop=True)\n",
    "    beta_star = beta_star.rename(index=_get_x_name)\n",
    "    assert beta_star.sum() == M, beta_star.sum()\n",
    "    return beta_star\n",
    "\n",
    "def __get_y(beta_star: pd.Series, X: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Generate y's, where beta_star indicates whether a column is relevant.\"\"\"\n",
    "    # pick out the relevant x's, zero out the rest\n",
    "    y = X * beta_star\n",
    "    # y is a simple sum of the relevant raw-data feeds\n",
    "    y = y.sum(axis=\"columns\")\n",
    "    # add noise\n",
    "    y = y + stats.norm.rvs(scale=NOISE, size=T)\n",
    "    y.name = \"y\"\n",
    "    return y\n",
    "\n",
    "\n",
    "def _gen_data_subsample(beta_star: pd.Series) -> DataSubsample:\n",
    "    \"\"\"Get X, y.\"\"\"\n",
    "    X = __gen_X()\n",
    "    y = __get_y(beta_star=beta_star, X=X)\n",
    "    return X, y\n",
    "\n",
    "def gen_data_sample() -> Tuple[pd.Series, DataSample]:\n",
    "    beta_star = _pick_relevant_x()\n",
    "    # train subsample\n",
    "    X, y = _gen_data_subsample(beta_star=beta_star)\n",
    "    train_subsample = X, y\n",
    "    # test subsample\n",
    "    X_, y_ = _gen_data_subsample(beta_star=beta_star)\n",
    "    test_subsample = X_, y_\n",
    "    sample = train_subsample, test_subsample\n",
    "    return beta_star, sample\n",
    "\n",
    "np.random.seed(1337)\n",
    "# M relevant x's, training subsample, testing subsample\n",
    "beta_star, ((X, y), (X_, y_)) = gen_data_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature creation\n",
    "\n",
    "To show how performance varies with increasing degree of parameterization, we need to increase the degree of parameterization. To do this in the linear regression setting, we can just increase the number of regressors until it equals---and indeed even further until it is much larger than---the number of observations.\n",
    "\n",
    "We'll do this in a very simple way: If we need `R` regressors, we'll use the first `R` columns of `X`. This doesn't matter: The `N` raw-data feeds were i.i.d. random, and which `M` of the `N` raw-data feeds were chosen to be relevant was also uniformly random. So shuffling the `N` raw-data feeds before choosing `R` of them is unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# this implementation uses\n",
    "# (a flavor of) random Fourier features,\n",
    "# which would be much more useful for actual prediction.\n",
    "# however, it's already getting to complicated\n",
    "# for my liking,\n",
    "# and IMO obscures the magic.\n",
    "\n",
    "def _get_rhs(X: pd.DataFrame) -> pd.Series:\n",
    "    w = stats.norm.rvs(size=len(X.columns))\n",
    "    w = pd.Series(w, index=X.columns)\n",
    "    rhs = X * w\n",
    "    rhs = rhs.sum(axis=\"columns\")\n",
    "    rhs = np.sin(rhs)\n",
    "    return rhs\n",
    "\n",
    "def get_RHS(X: pd.DataFrame, R: int) -> pd.DataFrame:\n",
    "    RHS = {_get_x_name(base=\"r\", n=r): _get_rhs(X=X) for r in range(R)}\n",
    "    RHS = pd.DataFrame(RHS)\n",
    "    return RHS\n",
    "\"\"\"\n",
    "\n",
    "def get_RHS(X: pd.DataFrame, R: int) -> pd.DataFrame:\n",
    "    return X.iloc[:, :R]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIDGE_REGULARIZATION = 1\n",
    "\n",
    "\n",
    "def __fit_star_(**kwargs) -> pd.Series:\n",
    "    \"\"\"Ignores args and returns beta_star.\"\"\"\n",
    "    return beta_star\n",
    "\n",
    "def __fit_ridgeless(y: pd.Series, RHS: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Like ridge regression, but with 0 regularization.\n",
    "    (X'X)^{-1} X'y, where ^{-1} denotes Moore-Penrose pseudoinverse.\n",
    "    This is OLS if len(X.columns) <= len(y).\n",
    "    \"\"\"\n",
    "    X_T_X = RHS.T @ RHS\n",
    "    X_T_X_inv = pinv(X_T_X)\n",
    "    X_T_y = RHS.T @ y\n",
    "    beta_hat = X_T_X_inv @ X_T_y\n",
    "    beta_hat = pd.Series(beta_hat, index=RHS.columns)\n",
    "    return beta_hat\n",
    "\n",
    "def __fit_ridge(\n",
    "        y: pd.Series, RHS: pd.DataFrame,\n",
    "        regularization: float=RIDGE_REGULARIZATION\n",
    "    ) -> pd.Series:\n",
    "    \"\"\"This is OLS if penalty == 0.\"\"\"\n",
    "    model = lm.Ridge(alpha=regularization, fit_intercept=False)\n",
    "    model = model.fit(y=y, X=RHS)\n",
    "    beta_hat = pd.Series(model.coef_, index=RHS.columns)\n",
    "    return beta_hat\n",
    "    \n",
    "\n",
    "def _fit_model(y: pd.Series, RHS: pd.DataFrame, kind: str=\"star\") -> pd.Series:\n",
    "    \"\"\"Get beta_hat.\"\"\"\n",
    "    _fit = {\n",
    "        \"star\": __fit_star_,\n",
    "        \"ridgeless\": __fit_ridgeless,\n",
    "        \"ridge\": __fit_ridge\n",
    "    }[kind]\n",
    "    beta_hat = _fit(y=y, RHS=RHS)\n",
    "    return beta_hat\n",
    "\n",
    "def fit_model(\n",
    "        y: pd.Series, X: pd.DataFrame, R: int, kind: str=\"star\"\n",
    "    ) -> pd.Series:\n",
    "    RHS = get_RHS(X=X, R=R)\n",
    "    beta_hat = _fit_model(y=y, RHS=RHS, kind=kind)\n",
    "    # fill the potentially-empty var's\n",
    "    beta_hat = beta_hat.reindex(index=X.columns).fillna(0)\n",
    "    return beta_hat\n",
    "\n",
    "def fit_model_(R: int, kind: str=\"star\"):\n",
    "    return fit_model(y=y, X=X, R=R, kind=kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_y_hat(beta: pd.Series, X: pd.DataFrame) -> pd.Series:\n",
    "    y_hat = X * beta\n",
    "    y_hat = y_hat.sum(axis=\"columns\")\n",
    "    return y_hat\n",
    "\n",
    "def __get_loss(y: pd.Series, y_hat: pd.Series) -> float:\n",
    "    \"\"\"RMSE.\"\"\"\n",
    "    loss = y_hat - y\n",
    "    loss = loss**2\n",
    "    loss = loss.mean()\n",
    "    loss = loss**0.5\n",
    "    return loss\n",
    "\n",
    "def _get_loss(y: pd.Series, X: pd.DataFrame, beta: pd.Series) -> float:\n",
    "    y_hat = __get_y_hat(beta=beta, X=X)\n",
    "    return __get_loss(y=y, y_hat=y_hat)\n",
    "\n",
    "def get_loss(\n",
    "        y: pd.Series, X: pd.DataFrame,  # train subsample\n",
    "        y_: pd.Series, X_: pd.Series,  # test subsample\n",
    "        R: int,\n",
    "        kind: str=\"star\"\n",
    "    ) -> float:\n",
    "    \"\"\"Fit specified model and get test-subsample loss.\"\"\"\n",
    "    beta_hat = fit_model(y=y, X=X, R=R, kind=kind)\n",
    "    loss = _get_loss(y=y_, X=X_, beta=beta_hat)\n",
    "    return loss\n",
    "\n",
    "def get_loss_(R: int, kind: str=\"star\") -> float:\n",
    "    return get_loss(y=y, X=X, y_=y_, X_=X_, R=R, kind=kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Implicit regularization is regularization\n",
    "\n",
    "Nevertheless, it shows the point: As $R \\to T$, that is, as the number of regressors gets very close to the number of observations, we suffer from a singularity issue, in particular when $R$ approaches $T$ from above. The RMSE of the OLS-type estimator blows up (`ridgeless loss`), and we see the canonical benefit of regularization (in this case, a no-thought ridge model) (`ridge loss`). (Although, notice, even the ridge model is still pretty bad.)\n",
    "\n",
    "Yet, when $R \\gg T$, that is, when the number of regressors is much greater than the number of observations, the OLS-type model actually becomes indistinguishable from the regularized model. So, thanks to the Moore-Penrose pseudoinverse, as we really crank up the complexity (dimensionality) of our model, we get a natural implicit regularization, without ever having to consciously choose any regularization parameters.\n",
    "\n",
    "\n",
    "## Benefit of aggressive overparameterization\n",
    "\n",
    "But again, here's the real shocking takeaway: Conventional wisdom might suggest that, despite being unique, the solutions to the overparameterized fits ($R \\gg T$) still won't hold up any better out-of-sample than the perfectly-parameterized fit ($R = T$). In both cases, we've (long-since) perfectly overfit the training data. But [the conventional wisdom is wrong](https://youtu.be/AiA1NVEf9K4)... we know for a fact that the ridgeless model will give zero training error as soon as $R \\geq T$ (and, if we make $R$ big enough, so will ridge, as it finally gets \"lucky\" with a small-norm $\\beta$ that fits the training data perfectly). Yet, the $R \\gg T$ cases give much, much smaller test error than the $R=T$ case!\n",
    "\n",
    "\n",
    "Note: In this oversimplified toy setup, the actual test-subsample performance of the models is almost indistinguishable from (indeed worse than) a model that simply ignores the $x$'s and guesses $0$ every time (`flat loss`), and nowhere near the performance of the ground-truth model (`star loss`). To actually get some good performance out of this, I think we could make the methodology closer to Bartlett's, e.g. use random Fourier features to have a better chance of \"blending in\" some of the predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=0:\n",
      "star loss = 0.9236567098968085\n",
      "flat loss = 34.854137423977086\n",
      "\n",
      "R=1:\n",
      "ridgeless loss = 34.69244635658993\n",
      "ridge loss = 34.6932738017274\n",
      "\n",
      "R=5:\n",
      "ridgeless loss = 36.662583109678685\n",
      "ridge loss = 36.63295490774653\n",
      "\n",
      "R=50:\n",
      "ridgeless loss = 49.33762691655742\n",
      "ridge loss = 48.37308734314992\n",
      "\n",
      "R=99:\n",
      "ridgeless loss = 343.2648322926529\n",
      "ridge loss = 68.69292052254256\n",
      "\n",
      "R=100:\n",
      "ridgeless loss = 2032.2315208423208\n",
      "ridge loss = 68.17625896099608\n",
      "\n",
      "R=101:\n",
      "ridgeless loss = 526.7897819844312\n",
      "ridge loss = 68.77494097796075\n",
      "\n",
      "R=550:\n",
      "ridgeless loss = 39.28105850975485\n",
      "ridge loss = 39.26210071222908\n",
      "\n",
      "R=1000:\n",
      "ridgeless loss = 37.04166829684086\n",
      "ridge loss = 37.036128880866926\n",
      "\n",
      "R=2000:\n",
      "ridgeless loss = 34.99921536904601\n",
      "ridge loss = 34.99878478966434\n",
      "\n",
      "R=5000:\n",
      "ridgeless loss = 34.58332111001222\n",
      "ridge loss = 34.61142971092663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"R=0:\")\n",
    "print(f\"star loss = {get_loss_(R=None)}\")\n",
    "print(f\"flat loss = {(y_**2).mean()**0.5}\")\n",
    "print()\n",
    "\n",
    "for R in (1, 5, int(T/2), T-1, T, T+1, int((T+M) /2), M, M*2, M*5):\n",
    "    print(f\"R={R}:\")\n",
    "    for kind in (\"ridgeless\", \"ridge\"):\n",
    "        print(f\"{kind} loss = {get_loss_(R=R, kind=kind)}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
